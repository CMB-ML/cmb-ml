{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was a bit difficult to explain any of them outside the context of others. Now that they've been explained [for single stages](demonstrations/E_CMB_ML_framework.ipynb) and [for the whole pipeline](demonstrations/F_CMB_ML_pipeline.ipynb) I can illustrate how I use them with less worry about overwhelming the reader.\n",
    "\n",
    "In this notebook, I want to tackle multiple simulations and splits. This should:\n",
    "- Drive home the advantage of using Assets with path_templates\n",
    "- Introduce the `Split` object\n",
    "- Show a few different structures for Executors\n",
    "- Point out a few other practices that I think are good <!--PLEASE GIVE FEEDBACK ON THIS. I'M MAKING THIS UP. I'M A CHILD LOST IN THE WOODS-->\n",
    "\n",
    "I'll cover three different Executors, which differ in how they process multiple simulations:\n",
    "- [A simple Executor](#serially-iterating-executor) that iterates slowly (relatively)\n",
    "- An Executor that uses multiprocessing to iterate quickly\n",
    "- An Executor that sets up and uses a PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I'll continue with the example of wanting to convert a power spectrum into a map. But this time, I'll consider that there's a single power spectrum and produce many realizations for hypothetical Training, Validation, and Test splits. (This is likely contrived. Some independent variable is needed so that a network is trained to prediction something, but this suffices for now.)\n",
    "\n",
    "To do this, I introduce another object: a `Split`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CMB-ML dataset is split into Training, Validation, and Test subsets. The size of each split is set in a [split.yaml](../cfg/splits/3-3.yaml) configuration file (there are many).\n",
    "```yaml\n",
    "name: \"3-3\"\n",
    "Train:\n",
    "  n_sims: 3\n",
    "Valid:\n",
    "  n_sims: 3\n",
    "  n_sims_cap: ${n_infer_cap}\n",
    "Test:\n",
    "  n_sims: 3\n",
    "  n_sims_cap: ${n_infer_cap}\n",
    "run_inference_on: ${run_inference_on}\n",
    "```\n",
    "\n",
    "- `name`: Used for file paths. \n",
    "  - For small, debugging, splits I will use 3-3 to mean three splits with three simulations each.\n",
    "  - For larger, more practical, splits I will use the total number of simulations (e.g., 1450 or 2450)\n",
    "- `Train` / `Valid` / `Test`: The name of a split\n",
    "  - CMB-ML can also use `Test1`, `Test2`, etc., if different test distributions are to be examined\n",
    "  - Use of other split names would need to be matched elsewhere in configurations an possibly in Python\n",
    "- `n_sims`: The total number of simultions in this split\n",
    "- `n_sims_cap`: When debugging, it is faster to run inference on a portion of the dataset only. This key enables that.\n",
    "  - `${n_infer_cap}` causes Hydra to interpolate the value (get it from elsewhere). In this case, it will look for `n_infer_cap` in the top level configuration. This is a shortcut I use so that the parameter can be in the correct place in the configuration YAMLs, but I can change it more easily at a single location.\n",
    "- `run_inference_on`: Whether to run Prediction (and later) stages on \"Test\" or \"Valid\" data. I try to use \"Test\" only for final runs, and \"Validation\" while developing.\n",
    "  - Again, this parameter is set at the top level, but I believe it \"belongs\" here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some stages of the pipeline may only apply to certain splits. For instance, when predicting results, I don't need to run on the training datasplit. In this case, the [pipeline YAML](../cfg/pipeline/pipe_model_patch_nn.yaml) has a key to note this:\n",
    "```yaml\n",
    "predict: &final_inference\n",
    "  assets_out:\n",
    "    cmb_map: \n",
    "      handler: HealpyMap\n",
    "      path_template: \"{root}/{dataset}/{working}{stage}/{split}/{sim}/cmb_map_pred_{epoch}.fits\"\n",
    "  assets_in:\n",
    "    obs_maps: {stage: make_sims}\n",
    "    lut: {stage: make_lut}\n",
    "    dataset_stats: {stage: get_dataset_stats}\n",
    "    model: {stage: train}\n",
    "  splits:\n",
    "    - ${splits.run_inference_on}\n",
    "  epochs: ${use_epochs}\n",
    "  dir_name: PatchNN_F_Predict\n",
    "  make_stage_log: True\n",
    "```\n",
    "\n",
    "The lines\n",
    "```yaml\n",
    "  splits:\n",
    "    - ${splits.run_inference_on}\n",
    "```\n",
    "tell the Executor to only run on the split marked that way in the splits YAML.\n",
    "\n",
    "Similar patterns can be seens elsewhere. For instance, the preprocessing stage for that pipeline is only applied to the Training and Validation datasplits; in this particular case, preprocessing of the Test datasplit occurs during inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Split` object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle splits within executors, we have the `Split` class. An instance of this class is created for each split. It retains the name and number of simulations. It could be extended to track additional variables. It also has a method, `iter_sims()` that produces an iterator over all simulations within the split (referring to each simulation as an integer index). Examples of this will be provided below.\n",
    "\n",
    "<!-- Generally, splitting of a dataset is done by producing one full dataset and randomly assigning elements to the different splits. The current system does this in effect, because it draws each simulation at random from a hypothetical source distribution. I do generate them pre-labelled into a category, but this is immaterial. -->\n",
    "\n",
    "<!-- When the project was started, I wanted to investigate the impact of different distributions on both training and inference.\n",
    "This may be picked up again in the future. \n",
    "There are vestiges of this intention in the code, such as the ps_fidu_fixed parameter within a split. This would enable me to use a set of simulations with a single fiducial power spectrum, checking where a trained model struggles. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have to again set a few things up so that the CMB-ML framework plays nice with Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore this cell. \n",
    "# It's needed for the notebook to work, not something to learn.\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Set the local_system\n",
    "os.environ[\"CMB_ML_LOCAL_SYSTEM\"] = \"generic_lab\"\n",
    "\n",
    "# Add the path to the parent directory so I can import cmb-ml\n",
    "repo_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.insert(0, repo_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from hydra import compose, initialize\n",
    "import numpy as np\n",
    "import healpy as hp\n",
    "\n",
    "from cmbml.core import PipelineContext\n",
    "from cmbml.core import BaseStageExecutor, Asset\n",
    "from cmbml.core.asset_handlers import TextPowerSpectrum, HealpyMap, Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"F_Tutorial\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Outside of a notebook, Hydra will handle the logging. \n",
    "handler = logging.StreamHandler()  # StreamHandler sends logs to sys.stdout by default\n",
    "handler.setLevel(logging.DEBUG)\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from hydra.core.hydra_config import HydraConfig\n",
    "\n",
    "# There are better ways to do this outside of notebooks.\n",
    "with initialize(version_base=None, config_path=\"../cfg\"):\n",
    "    cfg = compose(config_name=\"config_demoG_framework\", return_hydra_config=True)\n",
    "    HydraConfig.instance().set_config(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Executor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first stage in this demonstration pipeline will produce a single power spectrum and save it to disk. I've made some adjustments from the previous version of this executor ([this notebook](demonstrations/E_CMB_ML_framework.ipynb))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakePSExecutor(BaseStageExecutor):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg, stage_str=\"ps_setup\")\n",
    "\n",
    "        self.out_cmb_ps: Asset = self.assets_out[\"cmb_ps\"]\n",
    "        out_cmb_ps_handler: TextPowerSpectrum  # Reference to the handler\n",
    "\n",
    "        # \"Power spectrum model\" as a parameter\n",
    "        self.ps_model = cfg.model.ps\n",
    "        self.ells = cfg.model.max_ell\n",
    "\n",
    "    def execute(self):\n",
    "        # The following logging line is always used at the start of an execute() method:\n",
    "        logger.debug(f\"Running {self.__class__.__name__} execute()\")\n",
    "\n",
    "        ell = np.arange(self.ells)\n",
    "        # Create the power spectrum from the model\n",
    "        ps = np.poly1d(self.ps_model)\n",
    "        self.out_cmb_ps.write(data=ps(ell))\n",
    "        logger.debug(f\"CMB power spectrum written to {self.out_cmb_ps.path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One change is to include a reference to the handler used for the asset. The line:\n",
    "```python\n",
    "out_cmb_ps_handler: TextPowerSpectrum\n",
    "```\n",
    "doesn't have any impact on the way the code runs. Python sees that the variable is never used, so there's only negligible computational cost to including it. However, it has a huge benefit to me as I write an Executor: I can very quickly find the code for the AssetHandler using my IDE. <!-- This pattern has a huge flaw: duplication. There's NOTHING that forces the handler listed to match the one in the config file. I have ideas for better solutions to this, but haven't implemented any. This reflects a short-coming of using Hydra, since the class of the asset handler isn't in a form IDE's currently recognize -->\n",
    "\n",
    "In this case, I'm getting the power spectrum as a polynomial model fit to a small range of the data. Previously, those were hard-coded values. Here, I've remove the hard-coding. This gives me more flexibility. Instead, the spectrum is included as a parameter from the configuration file $^\\dagger$. When creating executors I strive to **set variables in the __init__** method, pulling values from the configurations. \n",
    "\n",
    "Early on when I was putting together CMB-ML, I simply had\n",
    "```python\n",
    "self.cfg = cfg  # Do not do this!\n",
    "```\n",
    "in my executors. This defers pulling what I need from the configurations until the `execute()` function. It seems simpler, but really bit me when a reference to the configuration was incorrect (e.g. \"cfg.model.power_spectrum\" instead of \"cfg.ps_model\"). That would cause the executor to crash. Instead, by setting class variables for everything I need in the `__init__()` method, I fail faster and simplify debugging. <!-- ideally, I would take this to a next level, and have more extensive validation of the Hydra configs -->\n",
    "\n",
    "I've also added the use of `logger`. I try to use it sparingly.\n",
    "\n",
    "$^\\dagger$ This is not the correct way to generate a power spectrum, it's just what's suitable for a small demonstration. Also, the model is more \"data\" than a \"parameter\"; hard-coding it into the configuration isn't the best idea. Instead, I'd prefer to save it to a text file and use an Asset to manage the values. That makes this more extensible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I check each stage to ensure they work individually. I often change parameters in my configs for \"debug\" settings while doing so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shorter version of next cell, but less useful output b/c logging...\n",
    "# executor = MakePSExecutor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running MakePSExecutor execute()\n",
      "CMB power spectrum written to /data/jim/CMB_Data/Datasets2/DemoNotebook/A_PS_Setup/cmb_dummy_ps.fits\n",
      "Skipping stage logs for stage MakePSExecutor.\n",
      "Pipeline completed.\n"
     ]
    }
   ],
   "source": [
    "pipeline_context = PipelineContext(cfg)\n",
    "\n",
    "pipeline_context.add_pipe(MakePSExecutor)\n",
    "\n",
    "pipeline_context.prerun_pipeline()\n",
    "\n",
    "try:\n",
    "    pipeline_context.run_pipeline()\n",
    "except Exception as e:\n",
    "    # I typically use the logging library for these messages\n",
    "    logger.warning(\"An exception occured during the pipeline.\", exc_info=e)\n",
    "    raise e\n",
    "finally:\n",
    "    logger.info(\"Pipeline completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serially Iterating Executor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest form for an Executor will iterate using for-loops. Recall the previous version of the Executor:\n",
    "\n",
    "```python\n",
    "class PS2MapExecutor(BaseStageExecutor):\n",
    "    def __init__(self, cfg) -> None:\n",
    "        super().__init__(cfg, stage_str=\"ps2map\")\n",
    "\n",
    "        self.out_map_asset = self.assets_out[\"cmb_map\"]\n",
    "        self.in_ps_asset = self.assets_in[\"cmb_ps\"]\n",
    "\n",
    "    def execute(self) -> None:\n",
    "        ps = self.in_ps_asset.read()\n",
    "        print(f\"Power spectrum read from {self.out_map_asset.path}\")\n",
    "        cmb = hp.synfast(ps, nside=256)\n",
    "        self.out_map_asset.write(data=cmb)\n",
    "        print(f\"Map written to {self.out_map_asset.path}\")\n",
    "        return\n",
    "```\n",
    "\n",
    "This produced a single output map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That map was written without keeping track of units. I prefer to do so, so I need to use PySM3's units module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysm3.units as u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm now extending the method, producing multiple simulations for different data splits. Here's how I do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PS2MapExecutor(BaseStageExecutor):\n",
    "    def __init__(self, cfg) -> None:\n",
    "        super().__init__(cfg, stage_str=\"ps2map\")\n",
    "\n",
    "        self.out_map_asset = self.assets_out[\"cmb_map\"]\n",
    "        out_map_handler: HealpyMap  # Specify handler\n",
    "\n",
    "        self.in_ps_asset = self.assets_in[\"cmb_ps\"]\n",
    "        in_ps_handler: TextPowerSpectrum\n",
    "\n",
    "        # Set aside a placeholder for the power spectrum\n",
    "        self.ps = None\n",
    "\n",
    "    def execute(self) -> None:\n",
    "        logger.debug(f\"Running {self.__class__.__name__} execute()\")\n",
    "        # Load the power spectrum, just once\n",
    "        self.ps = self.in_ps_asset.read()\n",
    "        logger.debug(f\"Power spectrum read from {self.in_ps_asset.path}\")\n",
    "\n",
    "        for split in self.splits:\n",
    "            logger.debug(f\"Working on split {split.name}\")\n",
    "            with self.name_tracker.set_context(\"split\", split.name):\n",
    "                self.process_split(split)\n",
    "\n",
    "    def process_split(self, split):\n",
    "        for sim_num in split.iter_sims():\n",
    "            logger.debug(f\"Working on sim {sim_num:04d}\")\n",
    "            with self.name_tracker.set_context(\"sim_num\", sim_num):\n",
    "                self.process_sim()\n",
    "\n",
    "    def process_sim(self):\n",
    "        cmb = hp.synfast(self.ps, nside=256)\n",
    "        cmb = cmb * u.uK_CMB\n",
    "        self.out_map_asset.write(data=cmb)\n",
    "        logger.debug(f\"Map written to {self.out_map_asset.path}\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The changes:\n",
    "- Handlers for input and output Assets are specified.\n",
    "- `self.ps` is created and set to None in the `__init__()` method.\n",
    "  - Creating new instance variables outside of initialization is poor practice.\n",
    "  - I don't load the data during initialization; reading data (especially maps) will slow down the `pipeline_prerun()`.\n",
    "- The power spectrum is read at the start of `execute()`. \n",
    "  - It's the same data for all simulations; I don't want to read it multiple times\n",
    "  - I wait until execution to read data from files <!-- I do kind of want to check for file existence when possible... but that's something for another day. When creating the input Assets in the base executor... I think I check each asset to see if the path exists. Most require a reference to a {split} or {sim} or whatever, so I'd wrap it in a try: block and pass on exceptions where the key doesn't exist... -->\n",
    "- The main body of `execute()` is a loop over all splits.\n",
    "  - The splits were automatically set up by the base class.\n",
    "- A `process_split()` method does all work required for a split.\n",
    "  - This is a simple case, so only iteration over simulations is needed.\n",
    "  - There are cases where special processing is needed per split.\n",
    "- A `process_sim()` method does the work for each simulation, in the same way as was done originally.\n",
    "- `logger.debug()` is used.\n",
    "  - It is overused here, only for demonstration purposes. I try to avoid putting too much into the console and logs. In the slowest stages, I'm more likely to use it per simulation; other times only per split. In very fast stages, I do not use it other than at the entrance to `execute()`.\n",
    "\n",
    "In general, I enter a context right after the for-loop, outside the following method. This way, the method can remain ignorant of the wider context; this reduces parameters passed and tidies the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running MakePSExecutor execute()\n",
      "CMB power spectrum written to /data/jim/CMB_Data/Datasets2/DemoNotebook/A_PS_Setup/cmb_dummy_ps.fits\n",
      "Skipping stage logs for stage MakePSExecutor.\n",
      "Running PS2MapExecutor execute()\n",
      "Power spectrum read from /data/jim/CMB_Data/Datasets2/DemoNotebook/A_PS_Setup/cmb_dummy_ps.fits\n",
      "Working on split Train\n",
      "Working on sim 0000\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0000/cmb_dummy_map.fits\n",
      "Working on sim 0001\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0001/cmb_dummy_map.fits\n",
      "Working on sim 0002\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0002/cmb_dummy_map.fits\n",
      "Working on sim 0003\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0003/cmb_dummy_map.fits\n",
      "Working on sim 0004\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0004/cmb_dummy_map.fits\n",
      "Working on sim 0005\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0005/cmb_dummy_map.fits\n",
      "Working on sim 0006\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0006/cmb_dummy_map.fits\n",
      "Working on sim 0007\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0007/cmb_dummy_map.fits\n",
      "Working on sim 0008\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0008/cmb_dummy_map.fits\n",
      "Working on sim 0009\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0009/cmb_dummy_map.fits\n",
      "Working on sim 0010\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0010/cmb_dummy_map.fits\n",
      "Working on sim 0011\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0011/cmb_dummy_map.fits\n",
      "Working on sim 0012\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0012/cmb_dummy_map.fits\n",
      "Working on sim 0013\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0013/cmb_dummy_map.fits\n",
      "Working on sim 0014\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0014/cmb_dummy_map.fits\n",
      "Working on sim 0015\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0015/cmb_dummy_map.fits\n",
      "Working on sim 0016\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0016/cmb_dummy_map.fits\n",
      "Working on sim 0017\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0017/cmb_dummy_map.fits\n",
      "Working on sim 0018\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0018/cmb_dummy_map.fits\n",
      "Working on sim 0019\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0019/cmb_dummy_map.fits\n",
      "Working on sim 0020\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0020/cmb_dummy_map.fits\n",
      "Working on sim 0021\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0021/cmb_dummy_map.fits\n",
      "Working on sim 0022\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0022/cmb_dummy_map.fits\n",
      "Working on sim 0023\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0023/cmb_dummy_map.fits\n",
      "Working on sim 0024\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0024/cmb_dummy_map.fits\n",
      "Working on sim 0025\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0025/cmb_dummy_map.fits\n",
      "Working on sim 0026\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0026/cmb_dummy_map.fits\n",
      "Working on sim 0027\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0027/cmb_dummy_map.fits\n",
      "Working on sim 0028\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0028/cmb_dummy_map.fits\n",
      "Working on sim 0029\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0029/cmb_dummy_map.fits\n",
      "Working on sim 0030\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0030/cmb_dummy_map.fits\n",
      "Working on sim 0031\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0031/cmb_dummy_map.fits\n",
      "Working on sim 0032\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0032/cmb_dummy_map.fits\n",
      "Working on sim 0033\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0033/cmb_dummy_map.fits\n",
      "Working on sim 0034\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0034/cmb_dummy_map.fits\n",
      "Working on sim 0035\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0035/cmb_dummy_map.fits\n",
      "Working on sim 0036\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0036/cmb_dummy_map.fits\n",
      "Working on sim 0037\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0037/cmb_dummy_map.fits\n",
      "Working on sim 0038\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0038/cmb_dummy_map.fits\n",
      "Working on sim 0039\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0039/cmb_dummy_map.fits\n",
      "Working on sim 0040\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0040/cmb_dummy_map.fits\n",
      "Working on sim 0041\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0041/cmb_dummy_map.fits\n",
      "Working on sim 0042\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0042/cmb_dummy_map.fits\n",
      "Working on sim 0043\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0043/cmb_dummy_map.fits\n",
      "Working on sim 0044\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0044/cmb_dummy_map.fits\n",
      "Working on sim 0045\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0045/cmb_dummy_map.fits\n",
      "Working on sim 0046\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0046/cmb_dummy_map.fits\n",
      "Working on sim 0047\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0047/cmb_dummy_map.fits\n",
      "Working on sim 0048\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0048/cmb_dummy_map.fits\n",
      "Working on sim 0049\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0049/cmb_dummy_map.fits\n",
      "Working on sim 0050\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0050/cmb_dummy_map.fits\n",
      "Working on sim 0051\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0051/cmb_dummy_map.fits\n",
      "Working on sim 0052\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0052/cmb_dummy_map.fits\n",
      "Working on sim 0053\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0053/cmb_dummy_map.fits\n",
      "Working on sim 0054\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0054/cmb_dummy_map.fits\n",
      "Working on sim 0055\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0055/cmb_dummy_map.fits\n",
      "Working on sim 0056\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0056/cmb_dummy_map.fits\n",
      "Working on sim 0057\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0057/cmb_dummy_map.fits\n",
      "Working on sim 0058\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0058/cmb_dummy_map.fits\n",
      "Working on sim 0059\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0059/cmb_dummy_map.fits\n",
      "Working on sim 0060\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0060/cmb_dummy_map.fits\n",
      "Working on sim 0061\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0061/cmb_dummy_map.fits\n",
      "Working on sim 0062\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0062/cmb_dummy_map.fits\n",
      "Working on sim 0063\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0063/cmb_dummy_map.fits\n",
      "Working on sim 0064\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0064/cmb_dummy_map.fits\n",
      "Working on sim 0065\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0065/cmb_dummy_map.fits\n",
      "Working on sim 0066\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0066/cmb_dummy_map.fits\n",
      "Working on sim 0067\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0067/cmb_dummy_map.fits\n",
      "Working on sim 0068\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0068/cmb_dummy_map.fits\n",
      "Working on sim 0069\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0069/cmb_dummy_map.fits\n",
      "Working on sim 0070\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0070/cmb_dummy_map.fits\n",
      "Working on sim 0071\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0071/cmb_dummy_map.fits\n",
      "Working on sim 0072\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0072/cmb_dummy_map.fits\n",
      "Working on sim 0073\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0073/cmb_dummy_map.fits\n",
      "Working on sim 0074\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0074/cmb_dummy_map.fits\n",
      "Working on sim 0075\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0075/cmb_dummy_map.fits\n",
      "Working on sim 0076\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0076/cmb_dummy_map.fits\n",
      "Working on sim 0077\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0077/cmb_dummy_map.fits\n",
      "Working on sim 0078\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0078/cmb_dummy_map.fits\n",
      "Working on sim 0079\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0079/cmb_dummy_map.fits\n",
      "Working on sim 0080\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0080/cmb_dummy_map.fits\n",
      "Working on sim 0081\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0081/cmb_dummy_map.fits\n",
      "Working on sim 0082\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0082/cmb_dummy_map.fits\n",
      "Working on sim 0083\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0083/cmb_dummy_map.fits\n",
      "Working on sim 0084\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0084/cmb_dummy_map.fits\n",
      "Working on sim 0085\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0085/cmb_dummy_map.fits\n",
      "Working on sim 0086\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0086/cmb_dummy_map.fits\n",
      "Working on sim 0087\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0087/cmb_dummy_map.fits\n",
      "Working on sim 0088\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0088/cmb_dummy_map.fits\n",
      "Working on sim 0089\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0089/cmb_dummy_map.fits\n",
      "Working on sim 0090\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0090/cmb_dummy_map.fits\n",
      "Working on sim 0091\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0091/cmb_dummy_map.fits\n",
      "Working on sim 0092\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0092/cmb_dummy_map.fits\n",
      "Working on sim 0093\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0093/cmb_dummy_map.fits\n",
      "Working on sim 0094\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0094/cmb_dummy_map.fits\n",
      "Working on sim 0095\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0095/cmb_dummy_map.fits\n",
      "Working on sim 0096\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0096/cmb_dummy_map.fits\n",
      "Working on sim 0097\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0097/cmb_dummy_map.fits\n",
      "Working on sim 0098\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0098/cmb_dummy_map.fits\n",
      "Working on sim 0099\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0099/cmb_dummy_map.fits\n",
      "Working on split Valid\n",
      "Working on sim 0000\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Valid/sim0000/cmb_dummy_map.fits\n",
      "Working on sim 0001\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Valid/sim0001/cmb_dummy_map.fits\n",
      "Working on sim 0002\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Valid/sim0002/cmb_dummy_map.fits\n",
      "Working on sim 0003\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Valid/sim0003/cmb_dummy_map.fits\n",
      "Working on sim 0004\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Valid/sim0004/cmb_dummy_map.fits\n",
      "Working on sim 0005\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Valid/sim0005/cmb_dummy_map.fits\n",
      "Working on sim 0006\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Valid/sim0006/cmb_dummy_map.fits\n",
      "Working on sim 0007\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Valid/sim0007/cmb_dummy_map.fits\n",
      "Working on sim 0008\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Valid/sim0008/cmb_dummy_map.fits\n",
      "Working on sim 0009\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Valid/sim0009/cmb_dummy_map.fits\n",
      "Working on sim 0010\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Valid/sim0010/cmb_dummy_map.fits\n",
      "Working on sim 0011\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Valid/sim0011/cmb_dummy_map.fits\n",
      "Working on sim 0012\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Valid/sim0012/cmb_dummy_map.fits\n",
      "Working on sim 0013\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Valid/sim0013/cmb_dummy_map.fits\n",
      "Working on sim 0014\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Valid/sim0014/cmb_dummy_map.fits\n",
      "Working on sim 0015\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Valid/sim0015/cmb_dummy_map.fits\n",
      "Working on sim 0016\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Valid/sim0016/cmb_dummy_map.fits\n",
      "Working on sim 0017\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Valid/sim0017/cmb_dummy_map.fits\n",
      "Working on sim 0018\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Valid/sim0018/cmb_dummy_map.fits\n",
      "Working on sim 0019\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Valid/sim0019/cmb_dummy_map.fits\n",
      "Working on split Test\n",
      "Working on sim 0000\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Test/sim0000/cmb_dummy_map.fits\n",
      "Working on sim 0001\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Test/sim0001/cmb_dummy_map.fits\n",
      "Working on sim 0002\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Test/sim0002/cmb_dummy_map.fits\n",
      "Working on sim 0003\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Test/sim0003/cmb_dummy_map.fits\n",
      "Working on sim 0004\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Test/sim0004/cmb_dummy_map.fits\n",
      "Working on sim 0005\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Test/sim0005/cmb_dummy_map.fits\n",
      "Working on sim 0006\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Test/sim0006/cmb_dummy_map.fits\n",
      "Working on sim 0007\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Test/sim0007/cmb_dummy_map.fits\n",
      "Working on sim 0008\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Test/sim0008/cmb_dummy_map.fits\n",
      "Working on sim 0009\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Test/sim0009/cmb_dummy_map.fits\n",
      "Working on sim 0010\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Test/sim0010/cmb_dummy_map.fits\n",
      "Working on sim 0011\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Test/sim0011/cmb_dummy_map.fits\n",
      "Working on sim 0012\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Test/sim0012/cmb_dummy_map.fits\n",
      "Working on sim 0013\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Test/sim0013/cmb_dummy_map.fits\n",
      "Working on sim 0014\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Test/sim0014/cmb_dummy_map.fits\n",
      "Working on sim 0015\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Test/sim0015/cmb_dummy_map.fits\n",
      "Working on sim 0016\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Test/sim0016/cmb_dummy_map.fits\n",
      "Working on sim 0017\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Test/sim0017/cmb_dummy_map.fits\n",
      "Working on sim 0018\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Test/sim0018/cmb_dummy_map.fits\n",
      "Working on sim 0019\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Test/sim0019/cmb_dummy_map.fits\n",
      "Pipeline completed.\n"
     ]
    }
   ],
   "source": [
    "# pe = PS2MapExecutor(cfg)\n",
    "# pe.execute()\n",
    "pipeline_context = PipelineContext(cfg)\n",
    "\n",
    "pipeline_context.add_pipe(MakePSExecutor)\n",
    "pipeline_context.add_pipe(PS2MapExecutor)\n",
    "\n",
    "pipeline_context.prerun_pipeline()\n",
    "\n",
    "try:\n",
    "    pipeline_context.run_pipeline()\n",
    "except Exception as e:\n",
    "    # I typically use the logging library for these messages\n",
    "    logger.warning(\"An exception occured during the pipeline.\", exc_info=e)\n",
    "    raise e\n",
    "finally:\n",
    "    logger.info(\"Pipeline completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Executor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next executor, I'll consider wanting to find the minimum and maximum values across all simulations. Some models work better when the input data is between 0 and 1. Because of the large sizes of the datasets, I can't simply use the built-in MinMax Scaler from scikit-learn. Instead, an Executor can scan all the maps and output a YAML file with the values.\n",
    "\n",
    "I'll set up my YAML for this stage of the pipeline first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assets_out:\n",
      "  cmb_map_min_max:\n",
      "    handler: Config\n",
      "    path_template: '{root}/{dataset}/{stage}/cmb_min_max.yaml'\n",
      "assets_in:\n",
      "  cmb_map:\n",
      "    stage: ps2map\n",
      "dir_name: C_Map_Min_Max\n",
      "splits:\n",
      "- train\n",
      "- valid\n",
      "make_stage_log: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(cfg.pipeline.get_map_min_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of the extensive logging shown in the previous executor, I'm going to use `tqdm` to show progress. Something something logger.debug statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll put together two alternatives. The first follows the same structure as the Serial Executor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SerialFindStatsExecutor(BaseStageExecutor):\n",
    "    def __init__(self, cfg) -> None:\n",
    "        super().__init__(cfg, stage_str=\"get_map_min_max\")\n",
    "\n",
    "        self.out_min_max = self.assets_out[\"cmb_map_min_max\"]\n",
    "        out_min_max_handler: Config\n",
    "\n",
    "        self.in_map_asset = self.assets_in[\"cmb_map\"]\n",
    "        in_map_handler: HealpyMap  # Specify handler\n",
    "\n",
    "        # Set aside a placeholder for the power spectrum\n",
    "        self.extremes = dict(vmin=None, vmax=None)\n",
    "\n",
    "    def execute(self) -> None:\n",
    "        logger.debug(f\"Running {self.__class__.__name__} execute()\")\n",
    "        # Load the power spectrum, just once\n",
    "        for split in self.splits:\n",
    "            with self.name_tracker.set_context(\"split\", split.name):\n",
    "                self.process_split(split)\n",
    "        self.out_min_max.write(data=self.extremes)\n",
    "        logger.debug(f\"Map min/max written to {self.out_min_max.path}\")\n",
    "\n",
    "    def process_split(self, split):\n",
    "        for sim_num in tqdm(split.iter_sims()):\n",
    "            with self.name_tracker.set_context(\"sim_num\", sim_num):\n",
    "                self.process_sim()\n",
    "\n",
    "    def process_sim(self):\n",
    "        cmb_map = self.in_map_asset.read()\n",
    "        vmin, vmax = cmb_map.min(), cmb_map.max()\n",
    "        if self.extremes[\"vmin\"] is None:\n",
    "            self.extremes[\"vmin\"] = vmin\n",
    "            self.extremes[\"vmax\"] = vmax\n",
    "        self.extremes[\"vmax\"] = max(self.extremes[\"vmax\"], vmax)\n",
    "        self.extremes[\"vmin\"] = min(self.extremes[\"vmin\"], vmin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can now run that to collect the values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running SerialFindStatsExecutor execute()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23f40f973cda4886a5ae7856ce3348f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8583f445c794d77a7ddd755eca04fa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map min/max written to /data/jim/CMB_Data/Datasets2/DemoNotebook/C_Map_Min_Max/cmb_min_max.yaml\n"
     ]
    }
   ],
   "source": [
    "executor = SerialFindStatsExecutor(cfg)\n",
    "executor.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll check what I got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vmax': <Quantity 19231.49445432 uK_CMB>,\n",
       " 'vmin': <Quantity -19273.7254489 uK_CMB>}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm_path = executor.out_min_max.path\n",
    "min_max = Config().read(mm_path)\n",
    "min_max  # These values are suspiciously large, the physics may be wrong here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is great, but if I run it with more larger scale maps, it will take quite a while. Since the operations occur in a single thread, I can speed this up greatly using multiprocessing.\n",
    "\n",
    "I do not know the best way to do this, but I've found a method that works. I welcome suggestions for better patterns.\n",
    "\n",
    "When using multiprocessing, I need to provide *immutable* data types to the function. That won't work with the Namer, which is stateful. Intead, I'll use multiprocessing to get the relevant statistics (min and max, in this case) for each simulation, then operate on those statistics at the end.\n",
    "\n",
    "The steps are:\n",
    "- iterates through all sims and builds out the \"instructions\" for each, as a `TaskTarget`\n",
    "- runs the first TaskTarget in the main thread (for debugging)\n",
    "- sets up multiprocessing and runs all TaskTargets\n",
    "- process the results of all the individual TaskTargets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I define two helper classes, both of which are NamedTuples (which are immutable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "from pathlib import Path\n",
    "\n",
    "from cmbml.core import GenericHandler\n",
    "\n",
    "class FrozenAsset(NamedTuple):\n",
    "    path: Path\n",
    "    handler: GenericHandler\n",
    "\n",
    "class TaskTarget(NamedTuple):\n",
    "    cmb_asset: FrozenAsset\n",
    "    split_name: str\n",
    "    sim_num: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration purposes, I'm going to just define the start of the Executor here, and build it out across a few code cells. At the end, there's a final version with everything in one place.\n",
    "\n",
    "The initialization method is mostly the same, changes:\n",
    "- .\n",
    "- ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelFindStatsExecutor(BaseStageExecutor):\n",
    "    def __init__(self, cfg) -> None:\n",
    "        super().__init__(cfg, stage_str=\"get_map_min_max\")\n",
    "\n",
    "        self.out_min_max = self.assets_out[\"cmb_map_min_max\"]\n",
    "        out_min_max_handler: Config\n",
    "\n",
    "        self.in_map_asset = self.assets_in[\"cmb_map\"]\n",
    "        in_map_handler: HealpyMap  # Specify handler\n",
    "\n",
    "        self.n_workers = cfg.n_workers\n",
    "        self.scale_scan_method = None\n",
    "        self.scale_sift_method = None\n",
    "\n",
    "    def execute(self) -> None:\n",
    "        logger.debug(f\"Running {self.__class__.__name__} execute().\")\n",
    "        # Tasks are items on a to-do list\n",
    "        #   For each simulation, we compare the prediction and target\n",
    "        #   A task contains labels, file names, and handlers for each sim\n",
    "        tasks = self.build_tasks()\n",
    "\n",
    "        # Run a single task outside multiprocessing to catch issues quickly.\n",
    "        self.try_a_task(self.scale_scan_method, tasks[0])\n",
    "\n",
    "        results_list = self.run_all_tasks(self.scale_scan_method, tasks)\n",
    "\n",
    "        results_summary = self.scale_sift_method(results_list)\n",
    "\n",
    "        self.out_min_max.write(data=results_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I build out all the tasks. I've written this method with extensive nesting. On another day I may have split this into a few methods. \n",
    "\n",
    "In this case, I simply grab the path and handler object for each. I do not perform any operations using them. The FrozenAsset is constructed with this information so that they can be called individually. This is another factor that went in to the design of CMB-ML's Assets and AssetHandlers.\n",
    "\n",
    "Also, notice that `set_contexts()` to set multiple things at the same time in my name_tracker. This has nothing to do with multi-processing, but it's difficult to concoct a demonstration of this in other contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tasks(self):\n",
    "    tasks = []\n",
    "    for split in self.splits:\n",
    "        for sim in split.iter_sims():\n",
    "            context = dict(split=split.name, sim_num=sim)\n",
    "            with self.name_tracker.set_contexts(contexts_dict=context):\n",
    "                cmb = self.in_map_asset\n",
    "                cmb = FrozenAsset(path=cmb.path, handler=cmb.handler)\n",
    "                \n",
    "                tasks.append(TaskTarget(cmb_asset=cmb,\n",
    "                                        split_name=split.name, \n",
    "                                        sim_num=sim))\n",
    "    return tasks\n",
    "\n",
    "ParallelFindStatsExecutor.build_tasks = build_tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I define the method that actually does the processing I need. This method is static; because it doesn't need the state of the Executor, it can operate in a thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_min_max(task_target: TaskTarget):\n",
    "    \"\"\"\n",
    "    Acts on a single simulation (TaskTarget) to find the max and min values\n",
    "        for each detector and field.\n",
    "    \"\"\"\n",
    "    cmb = task_target.cmb_asset\n",
    "    cmb_data = cmb.handler.read(cmb.path)\n",
    "\n",
    "    res = {'vmin': cmb_data.min(), \n",
    "           'vmax': cmb_data.max()}\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I define the class method that will run a test task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_a_task(self, process, task: TaskTarget):\n",
    "    \"\"\"\n",
    "    Get statistics for one sim (task) outside multiprocessing first, \n",
    "    to avoid painful debugging within multiprocessing.\n",
    "    \"\"\"\n",
    "    res = process(task)\n",
    "    if 'error' in res.keys():\n",
    "        raise Exception(res['error'])\n",
    "\n",
    "ParallelFindStatsExecutor.try_a_task = try_a_task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now another method that will run all the tasks. This uses the multiprocessing library, so we need a couple more imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_tasks(self, process, tasks):\n",
    "    # Use multiprocessing to search through sims in parallel\n",
    "    # A manager allows collection of information across separate threads\n",
    "    with Manager() as manager:\n",
    "        results = manager.list()\n",
    "        # The Pool sets up the individual processes. \n",
    "        # Set processes according to the capacity of your computer\n",
    "        with Pool(processes=self.n_workers) as pool:\n",
    "            # Each result is the output of \"process\" running on each of the tasks\n",
    "            for result in tqdm(pool.imap_unordered(process, tasks), total=len(tasks)):\n",
    "                results.append(result)\n",
    "        # Convert the results to a regular list after multiprocessing is complete\n",
    "        #     and before the scope of the manager ends\n",
    "        results_list = list(results)\n",
    "    # Use the out_report asset to write all results to disk\n",
    "    return results_list\n",
    "\n",
    "ParallelFindStatsExecutor.run_all_tasks = run_all_tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once I've gotten the min and max for each individual map, I need to aggregate the results to get the min and max across all simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sift_results(results_list):\n",
    "    \"\"\"\n",
    "    Combine results from multiprocessing into a single dictionary.\n",
    "    \"\"\"\n",
    "    res = dict(vmin=None, vmax=None)\n",
    "    for r in results_list:\n",
    "        res['vmin'] = r['vmin'] if res['vmin'] is None else min(res['vmin'], r['vmin'])\n",
    "        res['vmax'] = r['vmax'] if res['vmax'] is None else max(res['vmax'], r['vmax'])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because I've split everything out like this, I have to do another strange function assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ParallelFindStatsExecutor execute().\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87d3683edbf04cf8a27ece42c96edd44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "executor = ParallelFindStatsExecutor(cfg)\n",
    "executor.scale_scan_method = find_min_max\n",
    "executor.scale_sift_method = sift_results\n",
    "\n",
    "executor.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we see the same results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vmax': <Quantity 19231.49445432 uK_CMB>,\n",
       " 'vmin': <Quantity -19273.7254489 uK_CMB>}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm_path = executor.out_min_max.path\n",
    "min_max = Config().read(mm_path)\n",
    "min_max  # These values are suspiciously large, the physics may be wrong here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewriting this one last time, I have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool, Manager\n",
    "\n",
    "from cmbml.core import GenericHandler\n",
    "\n",
    "class FrozenAsset(NamedTuple):\n",
    "    path: Path\n",
    "    handler: GenericHandler\n",
    "\n",
    "class TaskTarget(NamedTuple):\n",
    "    cmb_asset: FrozenAsset\n",
    "    split_name: str\n",
    "    sim_num: str\n",
    "\n",
    "\n",
    "class ParallelFindStatsExecutor(BaseStageExecutor):\n",
    "    def __init__(self, cfg) -> None:\n",
    "        super().__init__(cfg, stage_str=\"get_map_min_max\")\n",
    "\n",
    "        self.out_min_max = self.assets_out[\"cmb_map_min_max\"]\n",
    "        out_min_max_handler: Config\n",
    "\n",
    "        self.in_map_asset = self.assets_in[\"cmb_map\"]\n",
    "        in_map_handler: HealpyMap  # Specify handler\n",
    "\n",
    "        self.n_workers = cfg.n_workers\n",
    "        self.scale_scan_method = find_min_max\n",
    "        self.scale_sift_method = sift_results\n",
    "\n",
    "    def execute(self) -> None:\n",
    "        logger.debug(f\"Running {self.__class__.__name__} execute().\")\n",
    "        # Tasks are items on a to-do list\n",
    "        #   For each simulation, we compare the prediction and target\n",
    "        #   A task contains labels, file names, and handlers for each sim\n",
    "        tasks = self.build_tasks()\n",
    "\n",
    "        # Run a single task outside multiprocessing to catch issues quickly.\n",
    "        self.try_a_task(self.scale_scan_method, tasks[0])\n",
    "\n",
    "        results_list = self.run_all_tasks(self.scale_scan_method, tasks)\n",
    "\n",
    "        results_summary = self.scale_sift_method(results_list)\n",
    "\n",
    "        self.out_min_max.write(data=results_summary)\n",
    "\n",
    "    def build_tasks(self):\n",
    "        tasks = []\n",
    "        for split in self.splits:\n",
    "            for sim in split.iter_sims():\n",
    "                context = dict(split=split.name, sim_num=sim)\n",
    "                with self.name_tracker.set_contexts(contexts_dict=context):\n",
    "                    cmb = self.in_map_asset\n",
    "                    cmb = FrozenAsset(path=cmb.path, handler=cmb.handler)\n",
    "                    \n",
    "                    tasks.append(TaskTarget(cmb_asset=cmb,\n",
    "                                            split_name=split.name, \n",
    "                                            sim_num=sim))\n",
    "        return tasks\n",
    "\n",
    "    def try_a_task(self, process, task: TaskTarget):\n",
    "        \"\"\"\n",
    "        Get statistics for one sim (task) outside multiprocessing first, \n",
    "        to avoid painful debugging within multiprocessing.\n",
    "        \"\"\"\n",
    "        res = process(task)\n",
    "        if 'error' in res.keys():\n",
    "            raise Exception(res['error'])\n",
    "\n",
    "    def run_all_tasks(self, process, tasks):\n",
    "        # Use multiprocessing to search through sims in parallel\n",
    "        # A manager allows collection of information across separate threads\n",
    "        with Manager() as manager:\n",
    "            results = manager.list()\n",
    "            # The Pool sets up the individual processes. \n",
    "            # Set processes according to the capacity of your computer\n",
    "            with Pool(processes=self.n_workers) as pool:\n",
    "                # Each result is the output of \"process\" running on each of the tasks\n",
    "                for result in tqdm(pool.imap_unordered(process, tasks), total=len(tasks)):\n",
    "                    results.append(result)\n",
    "            # Convert the results to a regular list after multiprocessing is complete\n",
    "            #     and before the scope of the manager ends\n",
    "            results_list = list(results)\n",
    "        # Use the out_report asset to write all results to disk\n",
    "        return results_list\n",
    "\n",
    "def find_min_max(task_target: TaskTarget):\n",
    "    \"\"\"\n",
    "    Acts on a single simulation (TaskTarget) to find the max and min values\n",
    "        for each detector and field.\n",
    "    \"\"\"\n",
    "    cmb = task_target.cmb_asset\n",
    "    cmb_data = cmb.handler.read(cmb.path)\n",
    "\n",
    "    res = {'vmin': cmb_data.min(), \n",
    "           'vmax': cmb_data.max()}\n",
    "    return res\n",
    "\n",
    "def sift_results(results_list):\n",
    "    \"\"\"\n",
    "    Combine results from multiprocessing into a single dictionary.\n",
    "    \"\"\"\n",
    "    res = dict(vmin=None, vmax=None)\n",
    "    for r in results_list:\n",
    "        res['vmin'] = r['vmin'] if res['vmin'] is None else min(res['vmin'], r['vmin'])\n",
    "        res['vmax'] = r['vmax'] if res['vmax'] is None else max(res['vmax'], r['vmax'])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ParallelFindStatsExecutor execute().\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0049295841934deaaa7bf13ba57cd1d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'vmax': <Quantity 19231.49445432 uK_CMB>,\n",
       " 'vmin': <Quantity -19273.7254489 uK_CMB>}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executor = ParallelFindStatsExecutor(cfg)\n",
    "executor.scale_scan_method = find_min_max\n",
    "executor.scale_sift_method = sift_results\n",
    "\n",
    "executor.execute()\n",
    "\n",
    "mm_path = executor.out_min_max.path\n",
    "min_max = Config().read(mm_path)\n",
    "min_max  # These values are suspiciously large, the physics may be wrong here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The speed-up would be more obvious if more processors or larger maps are used. I've set it to 6 workers in the configuration for compatibility reasons.\n",
    "\n",
    "It does come at the cost of being more difficult to debug.\n",
    "\n",
    "This would not be effective for the creation of CMB maps. The `SYNFAST` algorithm is already configured to use multiple processors, so multiprocessing would likely to slow things down. <!-- 75% confidence -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a PyTorch Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmb-ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
