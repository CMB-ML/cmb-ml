{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was a bit difficult to explain any of them outside the context of others. Now that they've been explained [for single stages](demonstrations/E_CMB_ML_framework.ipynb) and [for the whole pipeline](demonstrations/F_CMB_ML_pipeline.ipynb) I can illustrate how I use them with less worry about overwhelming the reader.\n",
    "\n",
    "In this notebook, I want to tackle multiple simulations and splits. This should:\n",
    "- Drive home the advantage of using Assets with path_templates\n",
    "- Introduce the `Split` object\n",
    "- Show a few different structures for Executors\n",
    "- Point out a few other practices that I think are good <!--PLEASE GIVE FEEDBACK ON THIS. I'M MAKING THIS UP. I'M A CHILD LOST IN THE WOODS-->\n",
    "\n",
    "I'll cover three different Executors, which differ in how they process multiple simulations:\n",
    "- [A simple Executor](#serially-iterating-executor) that iterates slowly (relatively)\n",
    "- An Executor that uses multiprocessing to iterate quickly\n",
    "- An Executor that sets up and uses a PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I'll continue with the example of wanting to convert a power spectrum into a map. But this time, I'll consider that there's a single power spectrum and produce many realizations for hypothetical Training, Validation, and Test splits. (This is likely contrived. Some independent variable is needed so that a network is trained to prediction something, but this suffices for now.)\n",
    "\n",
    "To do this, I introduce another object: a `Split`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CMB-ML dataset is split into Training, Validation, and Test subsets. The size of each split is set in a [split.yaml](../cfg/splits/3-3.yaml) configuration file (there are many).\n",
    "```yaml\n",
    "name: \"3-3\"\n",
    "Train:\n",
    "  n_sims: 3\n",
    "Valid:\n",
    "  n_sims: 3\n",
    "  n_sims_cap: ${n_infer_cap}\n",
    "Test:\n",
    "  n_sims: 3\n",
    "  n_sims_cap: ${n_infer_cap}\n",
    "run_inference_on: ${run_inference_on}\n",
    "```\n",
    "\n",
    "- `name`: Used for file paths. \n",
    "  - For small, debugging, splits I will use 3-3 to mean three splits with three simulations each.\n",
    "  - For larger, more practical, splits I will use the total number of simulations (e.g., 1450 or 2450)\n",
    "- `Train` / `Valid` / `Test`: The name of a split\n",
    "  - CMB-ML can also use `Test1`, `Test2`, etc., if different test distributions are to be examined\n",
    "  - Use of other split names would need to be matched elsewhere in configurations an possibly in Python\n",
    "- `n_sims`: The total number of simultions in this split\n",
    "- `n_sims_cap`: When debugging, it is faster to run inference on a portion of the dataset only. This key enables that.\n",
    "  - `${n_infer_cap}` causes Hydra to interpolate the value (get it from elsewhere). In this case, it will look for `n_infer_cap` in the top level configuration. This is a shortcut I use so that the parameter can be in the correct place in the configuration YAMLs, but I can change it more easily at a single location.\n",
    "- `run_inference_on`: Whether to run Prediction (and later) stages on \"Test\" or \"Valid\" data. I try to use \"Test\" only for final runs, and \"Validation\" while developing.\n",
    "  - Again, this parameter is set at the top level, but I believe it \"belongs\" here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some stages of the pipeline may only apply to certain splits. For instance, when predicting results, I don't need to run on the training datasplit. In this case, the [pipeline YAML](../cfg/pipeline/pipe_model_patch_nn.yaml) has a key to note this:\n",
    "```yaml\n",
    "predict: &final_inference\n",
    "  assets_out:\n",
    "    cmb_map: \n",
    "      handler: HealpyMap\n",
    "      path_template: \"{root}/{dataset}/{working}{stage}/{split}/{sim}/cmb_map_pred_{epoch}.fits\"\n",
    "  assets_in:\n",
    "    obs_maps: {stage: make_sims}\n",
    "    lut: {stage: make_lut}\n",
    "    dataset_stats: {stage: get_dataset_stats}\n",
    "    model: {stage: train}\n",
    "  splits:\n",
    "    - ${splits.run_inference_on}\n",
    "  epochs: ${use_epochs}\n",
    "  dir_name: PatchNN_F_Predict\n",
    "  make_stage_log: True\n",
    "```\n",
    "\n",
    "The lines\n",
    "```yaml\n",
    "  splits:\n",
    "    - ${splits.run_inference_on}\n",
    "```\n",
    "tell the Executor to only run on the split marked that way in the splits YAML.\n",
    "\n",
    "Similar patterns can be seens elsewhere. For instance, the preprocessing stage for that pipeline is only applied to the Training and Validation datasplits; in this particular case, preprocessing of the Test datasplit occurs during inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Split` object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle splits within executors, we have the `Split` class. An instance of this class is created for each split. It retains the name and number of simulations. It could be extended to track additional variables. It also has a method, `iter_sims()` that produces an iterator over all simulations within the split (referring to each simulation as an integer index). Examples of this will be provided below.\n",
    "\n",
    "<!-- Generally, splitting of a dataset is done by producing one full dataset and randomly assigning elements to the different splits. The current system does this in effect, because it draws each simulation at random from a hypothetical source distribution. I do generate them pre-labelled into a category, but this is immaterial. -->\n",
    "\n",
    "<!-- When the project was started, I wanted to investigate the impact of different distributions on both training and inference.\n",
    "This may be picked up again in the future. \n",
    "There are vestiges of this intention in the code, such as the ps_fidu_fixed parameter within a split. This would enable me to use a set of simulations with a single fiducial power spectrum, checking where a trained model struggles. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have to again set a few things up so that the CMB-ML framework plays nice with Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore this cell. \n",
    "# It's needed for the notebook to work, not something to learn.\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Set the local_system\n",
    "os.environ[\"CMB_ML_LOCAL_SYSTEM\"] = \"generic_lab\"\n",
    "\n",
    "# Add the path to the parent directory so I can import cmb-ml\n",
    "repo_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.insert(0, repo_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from hydra import compose, initialize\n",
    "import numpy as np\n",
    "import healpy as hp\n",
    "\n",
    "from cmbml.core import PipelineContext\n",
    "from cmbml.core import BaseStageExecutor, Asset\n",
    "from cmbml.core.asset_handlers import TextPowerSpectrum, HealpyMap, Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"F_Tutorial\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Outside of a notebook, Hydra will handle the logging. \n",
    "handler = logging.StreamHandler()  # StreamHandler sends logs to sys.stdout by default\n",
    "handler.setLevel(logging.DEBUG)\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from hydra.core.hydra_config import HydraConfig\n",
    "\n",
    "# There are better ways to do this outside of notebooks.\n",
    "with initialize(version_base=None, config_path=\"../cfg\"):\n",
    "    cfg = compose(config_name=\"config_demoG_framework\", return_hydra_config=True)\n",
    "    HydraConfig.instance().set_config(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Executor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first stage in this demonstration pipeline will produce a single power spectrum and save it to disk. I've made some adjustments from the previous version of this executor ([this notebook](demonstrations/E_CMB_ML_framework.ipynb))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakePSExecutor(BaseStageExecutor):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg, stage_str=\"ps_setup\")\n",
    "\n",
    "        self.out_cmb_ps: Asset = self.assets_out[\"cmb_ps\"]\n",
    "        out_cmb_ps_handler: TextPowerSpectrum  # Reference to the handler\n",
    "\n",
    "        # \"Power spectrum model\" as a parameter\n",
    "        self.ps_model = cfg.model.ps\n",
    "        self.ells = cfg.model.max_ell\n",
    "\n",
    "    def execute(self):\n",
    "        # The following logging line is always used at the start of an execute() method:\n",
    "        logger.debug(f\"Running {self.__class__.__name__} execute()\")\n",
    "\n",
    "        ell = np.arange(self.ells)\n",
    "        # Create the power spectrum from the model\n",
    "        ps = np.poly1d(self.ps_model)\n",
    "        self.out_cmb_ps.write(data=ps(ell))\n",
    "        logger.debug(f\"CMB power spectrum written to {self.out_cmb_ps.path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One change is to include a reference to the handler used for the asset. The line:\n",
    "```python\n",
    "out_cmb_ps_handler: TextPowerSpectrum\n",
    "```\n",
    "doesn't have any impact on the way the code runs. Python sees that the variable is never used, so there's only negligible computational cost to including it. However, it has a huge benefit to me as I write an Executor: I can very quickly find the code for the AssetHandler using my IDE. <!-- This pattern has a huge flaw: duplication. There's NOTHING that forces the handler listed to match the one in the config file. I have ideas for better solutions to this, but haven't implemented any. This reflects a short-coming of using Hydra, since the class of the asset handler isn't in a form IDE's currently recognize -->\n",
    "\n",
    "In this case, I'm getting the power spectrum as a polynomial model fit to a small range of the data. Previously, those were hard-coded values. Here, I've remove the hard-coding. This gives me more flexibility. Instead, the spectrum is included as a parameter from the configuration file $^\\dagger$. When creating executors I strive to **set variables in the __init__** method, pulling values from the configurations. \n",
    "\n",
    "Early on when I was putting together CMB-ML, I simply had\n",
    "```python\n",
    "self.cfg = cfg  # Do not do this!\n",
    "```\n",
    "in my executors. This defers pulling what I need from the configurations until the `execute()` function. It seems simpler, but really bit me when a reference to the configuration was incorrect (e.g. \"cfg.model.power_spectrum\" instead of \"cfg.ps_model\"). That would cause the executor to crash. Instead, by setting class variables for everything I need in the `__init__()` method, I fail faster and simplify debugging. <!-- ideally, I would take this to a next level, and have more extensive validation of the Hydra configs -->\n",
    "\n",
    "I've also added the use of `logger`. I try to use it sparingly.\n",
    "\n",
    "$^\\dagger$ This is not the correct way to generate a power spectrum, it's just what's suitable for a small demonstration. Also, the model is more \"data\" than a \"parameter\"; hard-coding it into the configuration isn't the best idea. Instead, I'd prefer to save it to a text file and use an Asset to manage the values. That makes this more extensible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I check each stage to ensure they work individually. I often change parameters in my configs for \"debug\" settings while doing so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shorter version of next cell, but less useful output b/c logging...\n",
    "# executor = MakePSExecutor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running MakePSExecutor execute()\n",
      "CMB power spectrum written to /data/jim/CMB_Data/Datasets2/DemoNotebook/A_PS_Setup/cmb_dummy_ps.fits\n",
      "Skipping stage logs for stage MakePSExecutor.\n",
      "Pipeline completed.\n"
     ]
    }
   ],
   "source": [
    "pipeline_context = PipelineContext(cfg)\n",
    "\n",
    "pipeline_context.add_pipe(MakePSExecutor)\n",
    "\n",
    "pipeline_context.prerun_pipeline()\n",
    "\n",
    "try:\n",
    "    pipeline_context.run_pipeline()\n",
    "except Exception as e:\n",
    "    # I typically use the logging library for these messages\n",
    "    logger.warning(\"An exception occured during the pipeline.\", exc_info=e)\n",
    "    raise e\n",
    "finally:\n",
    "    logger.info(\"Pipeline completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serially Iterating Executor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest form for an Executor will iterate using for-loops. Recall the previous version of the Executor:\n",
    "\n",
    "```python\n",
    "class PS2MapExecutor(BaseStageExecutor):\n",
    "    def __init__(self, cfg) -> None:\n",
    "        super().__init__(cfg, stage_str=\"ps2map\")\n",
    "\n",
    "        self.out_map_asset = self.assets_out[\"cmb_map\"]\n",
    "        self.in_ps_asset = self.assets_in[\"cmb_ps\"]\n",
    "\n",
    "    def execute(self) -> None:\n",
    "        ps = self.in_ps_asset.read()\n",
    "        print(f\"Power spectrum read from {self.out_map_asset.path}\")\n",
    "        cmb = hp.synfast(ps, nside=256)\n",
    "        self.out_map_asset.write(data=cmb)\n",
    "        print(f\"Map written to {self.out_map_asset.path}\")\n",
    "        return\n",
    "```\n",
    "\n",
    "This produced a single output map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That map was written without keeping track of units. I prefer to do so, so I need to use PySM3's units module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysm3.units as u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm now extending the method, producing multiple simulations for different data splits. Here's how I do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PS2MapExecutor(BaseStageExecutor):\n",
    "    def __init__(self, cfg) -> None:\n",
    "        super().__init__(cfg, stage_str=\"ps2map\")\n",
    "\n",
    "        self.out_map_asset = self.assets_out[\"cmb_map\"]\n",
    "        out_map_handler: HealpyMap  # Specify handler\n",
    "\n",
    "        self.in_ps_asset = self.assets_in[\"cmb_ps\"]\n",
    "        in_ps_handler: TextPowerSpectrum\n",
    "\n",
    "        # Set aside a placeholder for the power spectrum\n",
    "        self.ps = None\n",
    "\n",
    "    def execute(self) -> None:\n",
    "        logger.debug(f\"Running {self.__class__.__name__} execute()\")\n",
    "        # Load the power spectrum, just once\n",
    "        self.ps = self.in_ps_asset.read()\n",
    "        logger.debug(f\"Power spectrum read from {self.in_ps_asset.path}\")\n",
    "\n",
    "        for split in self.splits:\n",
    "            logger.debug(f\"Working on split {split.name}\")\n",
    "            with self.name_tracker.set_context(\"split\", split.name):\n",
    "                self.process_split(split)\n",
    "\n",
    "    def process_split(self, split):\n",
    "        for sim_num in split.iter_sims():\n",
    "            logger.debug(f\"Working on sim {sim_num:04d}\")\n",
    "            with self.name_tracker.set_context(\"sim_num\", sim_num):\n",
    "                self.process_sim()\n",
    "\n",
    "    def process_sim(self):\n",
    "        cmb = hp.synfast(self.ps, nside=256)\n",
    "        cmb = cmb * u.uK_CMB\n",
    "        self.out_map_asset.write(data=cmb)\n",
    "        logger.debug(f\"Map written to {self.out_map_asset.path}\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The changes:\n",
    "- Handlers for input and output Assets are specified.\n",
    "- `self.ps` is created and set to None in the `__init__()` method.\n",
    "  - Creating new instance variables outside of initialization is poor practice.\n",
    "  - I don't load the data during initialization; reading data (especially maps) will slow down the `pipeline_prerun()`.\n",
    "- The power spectrum is read at the start of `execute()`. \n",
    "  - It's the same data for all simulations; I don't want to read it multiple times\n",
    "  - I wait until execution to read data from files <!-- I do kind of want to check for file existence when possible... but that's something for another day. When creating the input Assets in the base executor... I think I check each asset to see if the path exists. Most require a reference to a {split} or {sim} or whatever, so I'd wrap it in a try: block and pass on exceptions where the key doesn't exist... -->\n",
    "- The main body of `execute()` is a loop over all splits.\n",
    "  - The splits were automatically set up by the base class.\n",
    "- A `process_split()` method does all work required for a split.\n",
    "  - This is a simple case, so only iteration over simulations is needed.\n",
    "  - There are cases where special processing is needed per split.\n",
    "- A `process_sim()` method does the work for each simulation, in the same way as was done originally.\n",
    "- `logger.debug()` is used.\n",
    "  - It is overused here, only for demonstration purposes. I try to avoid putting too much into the console and logs. In the slowest stages, I'm more likely to use it per simulation; other times only per split. In very fast stages, I do not use it other than at the entrance to `execute()`.\n",
    "\n",
    "In general, I enter a context right after the for-loop, outside the following method. This way, the method can remain ignorant of the wider context; this reduces parameters passed and tidies the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running MakePSExecutor execute()\n",
      "CMB power spectrum written to /data/jim/CMB_Data/Datasets2/DemoNotebook/A_PS_Setup/cmb_dummy_ps.fits\n",
      "Skipping stage logs for stage MakePSExecutor.\n",
      "Running PS2MapExecutor execute()\n",
      "Power spectrum read from /data/jim/CMB_Data/Datasets2/DemoNotebook/A_PS_Setup/cmb_dummy_ps.fits\n",
      "Working on split Train\n",
      "Working on sim 0000\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0000/cmb_dummy_map.fits\n",
      "Working on sim 0001\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0001/cmb_dummy_map.fits\n",
      "Working on sim 0002\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Train/sim0002/cmb_dummy_map.fits\n",
      "Working on split Valid\n",
      "Working on sim 0000\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Valid/sim0000/cmb_dummy_map.fits\n",
      "Working on sim 0001\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Valid/sim0001/cmb_dummy_map.fits\n",
      "Working on sim 0002\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Valid/sim0002/cmb_dummy_map.fits\n",
      "Working on split Test\n",
      "Working on sim 0000\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Test/sim0000/cmb_dummy_map.fits\n",
      "Working on sim 0001\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Test/sim0001/cmb_dummy_map.fits\n",
      "Working on sim 0002\n",
      "Map written to /data/jim/CMB_Data/Datasets2/DemoNotebook/B_CMB_Map/Test/sim0002/cmb_dummy_map.fits\n",
      "Pipeline completed.\n"
     ]
    }
   ],
   "source": [
    "# pe = PS2MapExecutor(cfg)\n",
    "# pe.execute()\n",
    "pipeline_context = PipelineContext(cfg)\n",
    "\n",
    "pipeline_context.add_pipe(MakePSExecutor)\n",
    "pipeline_context.add_pipe(PS2MapExecutor)\n",
    "\n",
    "pipeline_context.prerun_pipeline()\n",
    "\n",
    "try:\n",
    "    pipeline_context.run_pipeline()\n",
    "except Exception as e:\n",
    "    # I typically use the logging library for these messages\n",
    "    logger.warning(\"An exception occured during the pipeline.\", exc_info=e)\n",
    "    raise e\n",
    "finally:\n",
    "    logger.info(\"Pipeline completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Executor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next executor, I'll consider wanting to find the minimum and maximum values across all simulations. Some models work better when the input data is between 0 and 1. Because of the large sizes of the datasets, I can't simply use the built-in MinMax Scaler from scikit-learn. Instead, an Executor can scan all the maps and output a YAML file with the values.\n",
    "\n",
    "I'll set up my YAML for this stage of the pipeline first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assets_out:\n",
      "  cmb_map_min_max:\n",
      "    handler: Config\n",
      "    path_template: '{root}/{dataset}/{stage}/cmb_min_max.yaml'\n",
      "assets_in:\n",
      "  cmb_map:\n",
      "    stage: ps2map\n",
      "dir_name: C_Map_Min_Max\n",
      "splits:\n",
      "- train\n",
      "- valid\n",
      "make_stage_log: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(cfg.pipeline.get_map_min_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll put together two alternatives. The first follows the same structure as the Serial Executor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SerialFindStatsExecutor(BaseStageExecutor):\n",
    "    def __init__(self, cfg) -> None:\n",
    "        super().__init__(cfg, stage_str=\"get_map_min_max\")\n",
    "\n",
    "        self.out_min_max = self.assets_out[\"cmb_map_min_max\"]\n",
    "        out_min_max_handler: Config\n",
    "\n",
    "        self.in_map_asset = self.assets_in[\"cmb_map\"]\n",
    "        in_map_handler: HealpyMap  # Specify handler\n",
    "\n",
    "        # Set aside a placeholder for the power spectrum\n",
    "        self.extremes = dict(vmin=None, vmax=None)\n",
    "\n",
    "    def execute(self) -> None:\n",
    "        logger.debug(f\"Running {self.__class__.__name__} execute()\")\n",
    "        # Load the power spectrum, just once\n",
    "        for split in self.splits:\n",
    "            with self.name_tracker.set_context(\"split\", split.name):\n",
    "                self.process_split(split)\n",
    "        self.out_min_max.write(data=self.extremes)\n",
    "        logger.debug(f\"Map min/max written to {self.out_min_max.path}\")\n",
    "\n",
    "    def process_split(self, split):\n",
    "        for sim_num in split.iter_sims():\n",
    "            with self.name_tracker.set_context(\"sim_num\", sim_num):\n",
    "                self.process_sim()\n",
    "\n",
    "    def process_sim(self):\n",
    "        cmb_map = self.in_map_asset.read()\n",
    "        vmin, vmax = cmb_map.min(), cmb_map.max()\n",
    "        if self.extremes[\"vmin\"] is None:\n",
    "            self.extremes[\"vmin\"] = vmin\n",
    "            self.extremes[\"vmax\"] = vmax\n",
    "        self.extremes[\"vmax\"] = max(self.extremes[\"vmax\"], vmax)\n",
    "        self.extremes[\"vmin\"] = min(self.extremes[\"vmin\"], vmin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can now run that to collect the values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running SerialFindStatsExecutor execute()\n",
      "Map min/max written to /data/jim/CMB_Data/Datasets2/DemoNotebook/C_Map_Min_Max/cmb_min_max.yaml\n"
     ]
    }
   ],
   "source": [
    "executor = SerialFindStatsExecutor(cfg)\n",
    "executor.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll check what I got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vmax': <Quantity 16859.56342773 uK_CMB>,\n",
       " 'vmin': <Quantity -17327.1949806 uK_CMB>}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm_path = executor.out_min_max.path\n",
    "min_max = Config().read(mm_path)\n",
    "min_max  # These values are suspicious, but they're just placeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is great, but if I run it with more larger scale maps, it will take quite a while. Since the operations occur in a single thread, I can speed this up greatly using multiprocessing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmb-ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
